# Свод по проектам

Ссылка | Направление | Цель | Отработанные навыки | Используемые библиотеки
------------- |------------- |---------------- | ---------------- | -----------------------
[Определение стоимости автомобилей](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/RU/ML_car_price_predict_regression/ML_car_price_predict.ipynb) | Machine learning (градиентный бустинг, регрессия) | Обучить модель для определения рыночной стоимости автомобиля. | Выполнена предобработка (удалены дубликаты, замены пропуски). Проведено обучение и предсказание моделей: CatBoost, LightGBM, LinearRegression, Random Forrest  с параметрами по умолчанию и  с использованием  наборов гиперпараметров. Выбрана лучшая модель по результатам метрики RMSE  1525 и времени обучения. | `Pandas`, `NumPy`, `Sklearn`, `CatBoost`, `GridSearchCV`, '`LightGBM`, `CatBoost`, `Seaborn`, `OrdinalEncoder`, `OHE`
[Анализ токсичных комментариев](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/RU/ML_analysis_toxic_comments_classification/ML_texts_toxic_comments.ipynb) | NLP, Machine learning | Классифицировать комментарии для отправки негативных на модерацию. Определить и построить модель со значением метрики качества F1 не меньше 0.75. | На валидационной выборке модель CatBoostRegressor показаала резульата F1 0.764, на тестовой выборке 0.756. При выполнении проведена очистка текста с помощью регулярных выражений, лемматизация с POS tag, обучены три модели и выполнен подбор гиперпараметров  | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `NLTK`, `LogisticRegression`, `Matplotlib`, `Seaborn`, `WordNetLemmatizer`, `DecisionTreeClassifier`,`tf-idf`
[Прогнозирование заказов такси](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/RU/ML_Taxi_orders_predict/Taxi_orders_predict_ML.ipynb) | Machine learning (временные ряды, регрессия, предсказания) | Обучить модель для прогнозирования такси на следующий час | Проведен анализ для определения трендов по часам в течении дня и по дням. Проведено обучение и предсказание gоделей LogisticRegression , RandomForestRegressor, CatBoostRegressor с параметрами по умолчанию и наборов гиперпараметров. Выбрана лучшая модель по результатам метрики RMSE.  | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `GridSearchCV`, `LogisticRegression`, `Matplotlib`, `Seaborn`, `RandomForestRegressor`, `TimeSeriesSplit`
[Определение фэйковых и реальных твитов про катастрофы, проект Kaggle](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/RU/Kaggle_ML_NLP_Disaster_Tweets/Natural_Language_Processing_with_Disaster_Tweets.ipynb) | Machine learning, NLP | Обучить модель для прогнозирования определения принадлежности твита к фэйку или реальному, целевая метрика F1 | На тренировочной выборке модель LogicRegression показаала резульата F1 0.76, на тестовой выборке 0.79. При выполнении проведена очистка текста с помощью регулярных выражений, лемматизация с POS tag, обучены 4 модели и выполнен подбор гиперпараметров.   | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `NLTK`, `LogisticRegression`, `Matplotlib`, `Seaborn`, `WordNetLemmatizer`, `LightGBM`,`TfidfVectorizer` , `DecisionTree`
[Предсказение цены дома, Kaggle Project](https://github.com/AndreiShagin/AndreiShagin_Projects/blob/main/RU/Kaggle_ML_house_prices_predict/Kaggle_house_prices_predict.ipynb) | Machine learning, regression | Обучить модель для предсказение цены дома | На тестовых данных CatBoost получил 0.12 RMSE. Топ 13% решение. | `Pandas`, `NumPy`, `Sklearn`, `CatBoostRegressor`, `LGBMRegressor`, `DecisionTreeRegressor`, `Matplotlib`, `Seaborn`, `StackingRegressor`, `LinearRegression`,`GradientBoostingRegressor` , `RandomForestRegressor`, `GridSearchCV`, `RandomForestRegressor`
